---
layout: post
title: "2021-2022 CVPR OCR"
author: "Yafei Li"
categories: paper
tags: [papers, ZSL]
image: Cha1.png

---

# 2021-2022 CVPR OCR

# OCR

### TemplateğŸ‘‡

### <u>Scene Text Detection</u>

#### Abstract

#### Idea

#### Figure

#### Architecture

# *Text Detection*

### <u>Few Could Be Better Than All:Feature Sampling and Grouping for Scene Text Detection</u>

#### Abstract

*Recently, transformer-based methods have achieved promising progresses in object detection, as they can elim- inate the post-processes like NMS and enrich the deep rep- resentations. However, these methods cannot well cope with scene text due to its extreme variance of scales and aspect ratios. In this paper, we present a simple yet ef- fective transformer-based architecture for scene text detec- tion. Different from previous approaches that learn robust deep representations of scene text in a holistic manner, our method performs scene text detection based on a few rep- resentative features, which avoids the disturbance by back- ground and reduces the computational cost. Specifically, we first select a few representative features at all scales that are highly relevant to foreground text. Then, we adopt a transformer for modeling the relationship of the sam- pled features, which effectively divides them into reason- able groups. As each feature group corresponds to a text in- stance, its bounding box can be easily obtained without any post-processing operation. Using the basic feature pyra- mid network for feature extraction, our method consistently achieves state-of-the-art results on several popular datasets for scene text detection.*

#### Idea

ç®€å•æœ‰æ•ˆçš„åŸºäºTransformerçš„åœºæ™¯æ–‡æœ¬æ£€æµ‹æ¡†æ¶

~~ä»¥å¾€æ•´ä½“çš„å­¦ä¹ åœºæ™¯æ–‡æœ¬çš„é²æ£’æ·±åº¦è¡¨ç¤º~~  -> æ–°æ–¹æ³•åŸºäºä¸€äº›ä»£è¡¨æ€§ç‰¹å¾æ‰§è¡Œåœºæ™¯æ–‡æœ¬æ£€æµ‹ï¼Œé¿å…äº†èƒŒæ™¯çš„å¹²æ‰°å¹¶é™ä½äº†è®¡ç®—æˆæœ¬

åšæ³•ï¼š

+ å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ‰€æœ‰å°ºåº¦ä¸Šé€‰æ‹©ä¸€äº›ä¸å‰æ™¯æ–‡æœ¬é«˜åº¦ç›¸å…³çš„ä»£è¡¨æ€§ç‰¹å¾ã€‚
+ ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨å˜æ¢å™¨å¯¹é‡‡æ ·ç‰¹å¾çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œæœ‰æ•ˆåœ°å°†å®ƒä»¬åˆ’åˆ†ä¸ºåˆç†çš„ç»„ã€‚
+ ç”±äºæ¯ä¸ªç‰¹å¾ç»„å¯¹åº”ä¸€ä¸ªæ–‡æœ¬å®ä¾‹ï¼Œå› æ­¤æ— éœ€ä»»ä½•åå¤„ç†æ“ä½œå³å¯è½»æ¾è·å¾—å…¶è¾¹ç•Œæ¡†ã€‚
+ ä½¿ç”¨åŸºæœ¬ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œè¿›è¡Œç‰¹å¾æå–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”¨äºåœºæ™¯æ–‡æœ¬æ£€æµ‹çš„å‡ ä¸ªæµè¡Œæ•°æ®é›†ä¸Šå§‹ç»ˆå¦‚ä¸€åœ°å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

#### Figure

![](https://raw.githubusercontent.com/jianlai2600/IMAGE/main/img/202209231120644.png)

#### Architecture

+ It consists of a backbone network, a multi-scale feature sampling network, and a feature grouping network. 
+ Specifically, multi-scale feature maps are first produced from the backbone network. 
+ Next, a multi-scale text extractor is used to predict the confidence scores of the representative text regions at the pixel level. 
+ Then, we select text point features with top-N scores and concatenate them with position embeddings. 
+ After that, we adopt a transformer to model the relationship between the sampled features and implicitly group them into fine representations by the attention mechanism. 
+ Finally, the detection results are obtained from the prediction heads.



# *Text Recognition*

### <u>Open-Set Text Recognition via Character-Context Decoupling</u>

#### Abstract

*The open-set text recognition task is an emerging challenge that requires an extra capability to cognize novel characters during evaluation. We argue that a major cause of the limited performance for current methods is the confounding effect of contextual information over the visual information of individual characters. Under open-set scenarios, the intractable bias in contextual information can be passed down to visual information, consequently impairing the classification performance. In this paper, a Character-Context Decoupling framework is proposed to alleviate this problem by separating contextual informa- tion and character-visual information. Contextual informa- tion can be decomposed into temporal information and linguistic information. Here, temporal information that mod- els character order and word length is isolated with a detached temporal attention module. Linguistic information that models n-gram and other linguistic statistics is separated with a decoupled context anchor mechanism. A va- riety of quantitative and qualitative experiments show that our method achieves promising performance on open-set, zero-shot, and close-set text recognition datasets.*

#### Idea

open-set->è¯†åˆ«æ–°çš„å­—

è€æ–¹æ³•é—®é¢˜ï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¸­çš„biasï¼‰å¯¹å•ä¸ªå­—ç¬¦ä¿¡æ¯çš„æ··æ·†ä½œç”¨ï¼Œå½±å“å•ä¸ªå­—ç¬¦çš„åˆ†ç±»

**ä¸Šä¸‹æ–‡ä¿¡æ¯å¯ä»¥è¢«åˆ†è§£ä¸ºæ—¶åºä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯**

***Temporal information*** -> *models character order and word length* ->*isolated with a **detached temporal attention module***

***Linguistic information*** ->*models **n-gram** and other linguistic statistics* -> *separated with a **decoupled context anchor mechanism***

**[æœ¯è¯­]n-gram -> é¢„è®¡æˆ–è€…è¯„ä¼°ä¸€ä¸ªå¥å­æ˜¯å¦åˆç†**



**<u>*è¯¥æ–¹æ³•çš„å¦™å°±æ˜¯å°†contextä¸å­—ç¬¦è§£è—•ï¼Œä¸“æ³¨äºå•ä¸ªå­—çš„salience regionï¼Œè€Œè€æ–¹æ³•ä¼šå»çœ‹contextï¼Œè€Œcontextä¼šæœ‰åå½±å“*</u>**

![](https://raw.githubusercontent.com/jianlai2600/IMAGE/main/img/202209231427956.png)

#### Figure

![](https://raw.githubusercontent.com/jianlai2600/IMAGE/main/img/202209231431394.png)

#### Architecture

+ In the framework, visual representation of the sample and character templates are first extracted with the DSBN-Res45 Network
+ Then the Detached Temporal Attention module predicts the word length and samples visual features x[t] for each timestamp. 
+ The visual prediction is achieved by matching prototypes (attention-reduced template features) with the Open-set classifier. 
+ Finally, the visual prediction is adjusted with the Decoupled Context Anchor module, and no adjustment is conducted when there is intractable linguistic information under open-set scenarios.



### <u>Pushing the Performance Limit of Scene Text Recognizer without Human Annotation</u>

#### Abstract

*Scene text recognition (STR) attracts much attention over the years because of its wide application. Most methods train STR model in a fully supervised manner which re- quires large amounts of labeled data. Although synthetic data contributes a lot to STR, it suffers from the real-to- synthetic domain gap that restricts model performance. In this work, we aim to boost STR models by leveraging both synthetic data and the numerous real unlabeled images, ex- empting human annotation cost thoroughly. A robust con- sistency regularization based semi-supervised framework is proposed for STR, which can effectively solve the instabil- ity issue due to domain inconsistency between synthetic and real images. A character-level consistency regularization is designed to mitigate the misalignment between characters in sequence recognition. Extensive experiments on stan- dard text recognition benchmarks demonstrate the effective- ness of the proposed method. It can steadily improve exist- ing STR models, and boost an STR model to achieve new state-of-the-art results. To our best knowledge, this is the first consistency regularization based framework that ap- plies successfully to STR.*

#### Idea

STRä¾èµ–äºæ ‡æ³¨æ•°æ®ç„¶åç›‘ç£å­¦ä¹ ï¼Œå¾ˆç¹ç

è€æ–¹æ³•é—®é¢˜ï¼šåˆæˆæ•°æ®é›†å°½ç®¡å¾ˆå¥½ï¼Œä½†æ˜¯å—é™äºåˆæˆ-çœŸå®ä¹‹é—´çš„å·®è·ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½

æ–°æ–¹æ³•ï¼šä½¿ç”¨åˆæˆæ•°æ®å’Œæœªæ ‡æ³¨çœŸå®æ•°æ®

<u>**A robust consistency regularization based semi-supervised framework** is proposed for STR, which can effectively solve the instability issue due to domain inconsistency between synthetic and real images.</u>

#### Figure

![](https://raw.githubusercontent.com/jianlai2600/IMAGE/main/img/202209231510041.png)

#### Architecture

+ <u>take advantage of labeled synthetic data and unlabeled real data</u>
+ An asymmetric structure is designed with EMA and domain adaption to encourage a stable model training



### <u>SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization</u>

#### Abstract

*Recently self-supervised representation learning has drawn considerable attention from the scene text recogni- tion community. Different from previous studies using con- trastive learning, we tackle the issue from an alternative perspective, i.e., by formulating the representation learning scheme in a generative manner. Typically, the neighbor- ing image patches among one text line tend to have simi- lar styles, including the strokes, textures, colors, etc. Moti- vated by this common sense, we augment one image patch and use its neighboring patch as guidance to recover itself. Specifically, we propose a Similarity-Aware Normalization (SimAN) module to identify the different patterns and align the corresponding styles from the guiding patch. In this way, the network gains representation capability for distin- guishing complex patterns such as messy strokes and clut- tered backgrounds. Experiments show that the proposed SimAN significantly improves the representation quality and achieves promising performance. Moreover, we surpris- ingly find that our self-supervised generative network has impressive potential for data synthesis, text image editing, and font interpolation, which suggests that the proposed SimAN has a wide range of practical applications.*

#### Idea

#### Figure

#### Architecture



